{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7ecdc8ff",
   "metadata": {},
   "source": [
    "# Lab Session 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dfa257f",
   "metadata": {},
   "source": [
    "## Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb84fe6b",
   "metadata": {},
   "source": [
    "### Data Normalization Guide\n",
    "\n",
    "Normalization ensures that all features contribute equally to a machine learning model. The steps are as follows:\n",
    "\n",
    "1. **Import the required library**  \n",
    "   First, import the normalization tools from scikit-learn, such as Min-Max Scaler or Standard Scaler.\n",
    "\n",
    "`from sklearn.preprocessing import MinMaxScaler, StandardScaler`\n",
    "\n",
    "2. **Prepare your dataset and split it into training and testing sets**  \n",
    "   Split the dataset into training and testing sets before scaling. This prevents information from the test set from influencing the scaling parameters, which could lead to data leakage.\n",
    "\n",
    "`X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42, test_size = 0.2)`\n",
    "\n",
    "3. **Create a scaler object**  \n",
    "   Decide which scaling method is appropriate:\n",
    "   - **Min-Max scaling:** rescales feature values to a fixed range, usually [0,1].  \n",
    "   - **Z-score standardization:** centers features around zero with unit variance.\n",
    "\n",
    "`scaler = MinMaxScaler()`\n",
    "\n",
    "4. **Fit the scaler to the training data**  \n",
    "\n",
    "`scaler.fit(X_train)`\n",
    "\n",
    "5. **Transform the training data using the fitted scaler**  \n",
    "   Apply the scaling to the training data to normalize its features. The training data is now ready to be used for model training.\n",
    "\n",
    "`X_train_scaled = scaler.transform(X_train)`\n",
    "\n",
    "6. **Transform the testing data using the same scaler**  \n",
    "   Apply the scaler (fitted on the training data) to the testing data.\n",
    "   \n",
    "`X_test_scaled = scaler.transform(X_test)`\n",
    "\n",
    "7. **Use the scaled data for modeling**  \n",
    "   Train your machine learning model using the scaled training data and evaluate it on the scaled testing data. \n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f152899",
   "metadata": {},
   "source": [
    "## Exercise 1: Splitting + Normalization\n",
    "Given the following iris dataset, which aims at distinguishing between three different species of iris flower ('setosa' 'versicolor' 'virginica') based on their sepal and petal dimensions, as shown below when you run the example code. follow the normalization steps to normalize your data and call first 5 rows of your normalized data and compare it to non-normalized data, what difference can you notice?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e67ef743",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "# Load the dataset\n",
    "data = load_iris()\n",
    "X, y = data.data, data.target\n",
    "\n",
    "# Feature names and target\n",
    "feature_names = data.feature_names\n",
    "target_name = 'species'\n",
    "\n",
    "# Map numeric labels to species names\n",
    "species = data.target_names[y]\n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame(X, columns=feature_names)\n",
    "df[target_name] = species\n",
    "\n",
    "# Display the first 5 rows\n",
    "print(\"Iris dataset (first 5 rows):\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da732829",
   "metadata": {},
   "source": [
    "## Signal Conditioning Techniques "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed0c6568",
   "metadata": {},
   "source": [
    "### Signal Conditioning guide \n",
    "\n",
    "### 1. Amplification\n",
    "**Amplification** is the process of increasing the amplitude of a signal.  \n",
    "- Purpose: Make weak signals strong enough for further processing or measurement.  \n",
    "- Example: Multiplying a signal by a gain factor \\(G > 1\\) increases its amplitude.\n",
    "\n",
    "---\n",
    "\n",
    "### 2. Attenuation\n",
    "**Attenuation** is the process of reducing the amplitude of a signal.  \n",
    "- Purpose: Prevent signals from saturating measurement equipment or bring them into a measurable range.  \n",
    "- Example: Multiplying a signal by a gain factor \\(0 < G < 1\\) reduces its amplitude.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "### 3. Filtering  \n",
    "**Filtering** is about cleaning up your data by removing parts of the signal you don't care about (like slow drift or random noise) while keeping the important patterns.  \n",
    "- **Purpose:** Focus on the meaningful signal and make it easier to analyze.  \n",
    "- **Example:** A bandpass filter keeps only the frequencies in a chosen range (like 1–30 Hz), removing both very slow changes and very fast noise.\n",
    "\n",
    "#### Filtering application:\n",
    "\n",
    "`lowcut, highcut = 1.0, 30.0`  \n",
    "- **What this line does:** Sets the range of frequencies we want to keep — from 1 Hz up to 30 Hz.  \n",
    "- **Why we use it:** This tells the filter what counts as “useful signal” vs. “noise.”  \n",
    "  - The low cutoff (1 Hz) removes slow trends or constant offsets.  \n",
    "  - The high cutoff (30 Hz) removes very fast fluctuations that are probably just noise.  \n",
    "- **tip:** Pick these numbers based on your problem — if you expect slower patterns, lower the low cutoff; if you expect faster patterns, raise the high cutoff.\n",
    "\n",
    "---\n",
    "\n",
    "`nyq = 0.5 * fs`  \n",
    "- **What this line does:** Finds the Nyquist frequency, which is half the sampling rate.  \n",
    "- **Why we use it:** Filters expect cutoff frequencies to be between 0 and 1 (as a fraction of Nyquist).  \n",
    "- **tip:** Think of this step as scaling your real-world frequencies into a range the computer understands — it’s just a conversion step.\n",
    "\n",
    "---\n",
    "\n",
    "`b, a = sg.butter(4, [lowcut/nyq, highcut/nyq], btype='band')`  \n",
    "- **What this line does:** Designs the actual filter using the cutoff frequencies we defined.  \n",
    "- **Why we use it:**  \n",
    "  - **Butterworth** filters are smooth and don’t distort the size of your signal in the range you care about.  \n",
    "  - **Order = 4** controls how “sharp” the filter is. A higher order cuts noise more aggressively but might make the signal look too perfect or cause weird edges.  \n",
    "- **tip:** You can experiment with the filter order — higher values give a cleaner signal, but be careful not to over-filter and lose important details.\n",
    "\n",
    "----\n",
    "\n",
    "`filtered = sg.filtfilt(b, a, original)`  \n",
    "- **What this line does:** Runs the data through the filter twice (forward and backward) so the cleaned signal stays perfectly aligned with the original (no time shift).  \n",
    "- **Why we use it:**  `filtfilt` keeps peaks, spikes, and other features in the correct place while removing noise.  \n",
    "- **What to watch for:**  \n",
    "  - Works best on signals with enough data points (avoid very short signals).  \n",
    "  - Always check the plot — the filtered result should look smoother but still follow the same pattern as the original.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58dd7f4e",
   "metadata": {},
   "source": [
    "## Exercise 2: Signal Conditioning\n",
    "Run the following code representing a signal function of a noisy sine wave, then perform the explained three methods and visualize the impact of each technique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1416b37",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import signal as sg\n",
    "\n",
    "# Sampling setup\n",
    "fs = 1000  # Hz\n",
    "t = np.linspace(0, 1, fs, endpoint=False)\n",
    "\n",
    "# Original signal: sine + DC offset + random noise\n",
    "original = 2*np.sin(2*np.pi*5*t) + 0.5 + 0.3*np.random.randn(len(t))\n",
    "\n",
    "plt.figure(figsize=(10, 3))\n",
    "plt.plot(t, original, label='Original')\n",
    "plt.title(\"Original Noisy Sine Wave with DC Offset\")\n",
    "plt.xlabel(\"Time [s]\")\n",
    "plt.ylabel(\"Amplitude\")\n",
    "plt.xlim(0, 0.5)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96499073",
   "metadata": {},
   "source": [
    "# Data conditioning techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de542c72",
   "metadata": {},
   "source": [
    "## Exercise 3: Data Conditioning\n",
    "\n",
    "#### 3.1 Using the same signal function as before and with the help of the hands on guide below, perform FFT Decomposition and aggregation(min, max, mean) and produce a plot for each part.\n",
    "\n",
    "#### 3.2 run synchronization code then determine the target frequency achieved.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a193b70",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "## Data conditioning guide\n",
    "### 1. Decomposition\n",
    "Decomposition breaks down a signal into simpler components, often to analyze its **frequency content** or **time-frequency patterns**.\n",
    "\n",
    "\n",
    "### Fourier Transform Code Implementation\n",
    "\n",
    "```python\n",
    "from scipy.fft import fft, fftfreq  # Import FFT functions\n",
    "\n",
    "# Step 1: Get signal parameters\n",
    "N = len(original)                   # N = number of samples in time series\n",
    "```\n",
    "**What's happening:** We need to know the length of our signal because the FFT algorithm requires this for proper frequency bin calculation and normalization.\n",
    "\n",
    "```python\n",
    "# Step 2: Apply the Discrete Fourier Transform\n",
    "yf = fft(original)                  # yf = FFT of signal; returns complex numbers\n",
    "```\n",
    "**Theory connection:** This implements the discrete version of the continuous Fourier transform formula above. Each element in `yf` is a complex number containing:\n",
    "- **Real part:** represents the cosine component at that frequency\n",
    "- **Imaginary part:** represents the sine component at that frequency\n",
    "- **Magnitude:** `abs(yf[k])` gives the amplitude of frequency component k\n",
    "- **Phase:** `angle(yf[k])` gives the phase shift of frequency component k\n",
    "\n",
    "```python\n",
    "# Step 3: Generate corresponding frequency values\n",
    "xf = fftfreq(N, 1/fs)              # xf = frequency bins; 1/fs is time step\n",
    "```\n",
    "**What's happening:** \n",
    "- `fftfreq()` creates an array of frequencies corresponding to each FFT bin\n",
    "- `1/fs` is the sampling period (time between samples)\n",
    "- For sampling rate `fs`, frequencies range from 0 to `fs/2` (Nyquist frequency)\n",
    "\n",
    "```python\n",
    "# Step 4: Plot the single-sided amplitude spectrum\n",
    "plt.figure(figsize=(10, 3))\n",
    "plt.plot(xf[:N//2], 2.0/N * np.abs(yf[:N//2]))\n",
    "```\n",
    "\n",
    "**Breaking down the plotting code:**\n",
    "\n",
    "| Code Component | Mathematical Meaning | Why We Do This |\n",
    "|---|---|---|\n",
    "| `xf[:N//2]` | Take first half of frequency bins | FFT is symmetric; we only need positive frequencies |\n",
    "| `yf[:N//2]` | Take first half of FFT results | Corresponds to positive frequencies only |\n",
    "| `np.abs(yf[:N//2])` | $|X[k]| = \\sqrt{\\text{Re}(X[k])^2 + \\text{Im}(X[k])^2}$ | Convert complex numbers to magnitudes |\n",
    "| `2.0/N` | Normalization factor | Account for single-sided spectrum and sample count |\n",
    "\n",
    "**Why the `2.0/N` scaling?**\n",
    "- **`N`:** Normalizes for the number of samples (FFT spreads energy across N bins)\n",
    "- **`2.0`:** Compensates for throwing away negative frequencies (doubles the amplitude)\n",
    "- Result: True amplitude values that match the original signal components\n",
    "\n",
    "\n",
    "**tip:** Fourier is widely used because it reveals the dominant frequencies in a signal and is simple to compute with most libraries (e.g., NumPy’s `fft`). For signals with transient changes, wavelets can provide better time-localized frequency information.\n",
    "\n",
    "---\n",
    "\n",
    "### 2. Aggregation \n",
    "\n",
    "Aggregation reduces multiple data points into a single representative value within a sliding window, commonly used for:\n",
    "\n",
    "- **Mean** - Average value over the window, which reults in **smoothing**\n",
    "- **Median** - Middle value when sorted  \n",
    "- **Minimum / Maximum** - Extreme values in the window\n",
    "\n",
    "#### Aggregation Implementation\n",
    "\n",
    "**Step 1: Convert to pandas Series for window operations**\n",
    "```python\n",
    "import pandas as pd\n",
    "\n",
    "# Convert the signal to a pandas Series\n",
    "signal = pd.Series(original)\n",
    "```\n",
    "**Why:** Pandas provides efficient rolling window operations that are optimized for time series analysis.\n",
    "\n",
    "**Step 2: Define window size**\n",
    "```python\n",
    "window_size = 50  # Number of samples in each aggregation window\n",
    "```\n",
    "**Important:** Choose window size based on your signal characteristics:\n",
    "- **Smaller windows:** Preserve more detail, less smoothing\n",
    "- **Larger windows:** More smoothing, lose fine details\n",
    "\n",
    "**Step 3: Apply rolling aggregation functions**\n",
    "```python\n",
    "# Minimum aggregation\n",
    "min_signal = signal.rolling(window=window_size, min_periods=1).min()\n",
    "\n",
    "# Maximum aggregation (same pattern)\n",
    "max_signal = signal.rolling(window=window_size, min_periods=1).max()\n",
    "\n",
    "# Mean aggregation\n",
    "mean_signal = signal.rolling(window=window_size, min_periods=1).mean()\n",
    "\n",
    "# Median aggregation\n",
    "median_signal = signal.rolling(window=window_size, min_periods=1).median()\n",
    "```\n",
    "\n",
    "**Code breakdown:**\n",
    "- **`.rolling(window=window_size)`:** Creates sliding window of specified size\n",
    "- **`min_periods=1`:** Ensures calculation even when fewer than `window_size` points available (useful at signal boundaries)\n",
    "- **`.min()/.max()/.mean()/.median()`:** Applies the aggregation function to each window\n",
    "\n",
    "**tip:** Rolling aggregation is perfect for noise reduction and trend identification. Use min/max to find envelope boundaries, mean for general smoothing, and median for robust smoothing that's less affected by outliers.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "### 4. Interpolation\n",
    "Interpolation estimates values at points where data is missing or needs higher resolution.\n",
    "\n",
    "- **Linear interpolation:** Connects points with straight lines  \n",
    "- **Polynomial Interpolation:** Fits a polynomial through known data points  \n",
    "- **Splines:** Piecewise polynomials with smooth transitions at the joins  \n",
    "\n",
    "---\n",
    "\n",
    "### 5. Data Synchronization\n",
    "When combining datasets sampled at different frequencies, synchronization aligns them in time space. Techniques include **resampling**, **interpolation**, or **time-stamping alignment**.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc1af323",
   "metadata": {},
   "outputs": [],
   "source": [
    "# synchronization\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# -----------------------------\n",
    "# 1. Create sample datasets (15 seconds)\n",
    "# -----------------------------\n",
    "\n",
    "# Time series 1: sampled every 1 second (15 points)\n",
    "time1 = pd.date_range(start='2025-09-04 00:00:00', periods=15, freq='1s')\n",
    "data1 = np.sin(np.linspace(0, 3*np.pi, 15))  # sine wave data\n",
    "df1 = pd.DataFrame({'Time': time1, 'Value1': data1}).set_index('Time')\n",
    "\n",
    "# Time series 2: sampled every 3 seconds (6 points)\n",
    "time2 = pd.date_range(start='2025-09-04 00:00:00', periods=6, freq='3s')\n",
    "data2 = np.cos(np.linspace(0, 3*np.pi, 6))  # cosine wave data\n",
    "df2 = pd.DataFrame({'Time': time2, 'Value2': data2}).set_index('Time')\n",
    "\n",
    "# -----------------------------\n",
    "# 2. Visualize original data\n",
    "# -----------------------------\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.plot(df1.index, df1['Value1'], 'o-', label='Value1 (1s freq)')\n",
    "plt.plot(df2.index, df2['Value2'], 's-', label='Value2 (3s freq)')\n",
    "plt.title('Original Time Series with Different Frequencies')\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Value')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# -----------------------------\n",
    "# 3. Synchronize data\n",
    "# -----------------------------\n",
    "# Resample df2 to 1-second frequency and interpolate\n",
    "df2_sync = df2.resample('1s').interpolate(method='linear')\n",
    "\n",
    "# Merge the datasets\n",
    "df_sync = df1.join(df2_sync)\n",
    "\n",
    "# -----------------------------\n",
    "# 4. Visualize synchronized data\n",
    "# -----------------------------\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.plot(df_sync.index, df_sync['Value1'], 'o-', label='Value1 (1s freq)')\n",
    "plt.plot(df_sync.index, df_sync['Value2'], 's-', label='Value2 synchronized')\n",
    "plt.title('Synchronized Time Series (1s frequency)')\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Value')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf160701",
   "metadata": {},
   "source": [
    "# Handling Missing Values in Data\n",
    "\n",
    "\n",
    "\n",
    "## Exercise 4: Handling missing values\n",
    "run the following code cell that contains a dataset with several missing values (NaNs), and make use of the Missing values guide to perform the following tasks\n",
    "\n",
    "\n",
    "### 1. Explore Missing Values\n",
    "- Check which columns or rows have missing values.\n",
    "\n",
    "\n",
    "### 2. Implement Your Solution\n",
    "- Apply deletion or imputation based on your strategy.\n",
    "\n",
    "\n",
    "### 3. Verify\n",
    "- Check that no missing values remain.\n",
    "- Observe how your changes affected the dataset.\n",
    "\n",
    "---\n",
    "\n",
    "## Missing values guide \n",
    "\n",
    "\n",
    "In real-world datasets, we often encounter **missing values** (NaNs). Dealing with them is important because many machine learning algorithms cannot handle NaNs directly.\n",
    "We typically handle missing values in **two main ways**: **deletion** and **imputation**.\n",
    "\n",
    "## 1. Deletion\n",
    "\n",
    "Deletion means removing the missing values from the dataset. There are several approaches:\n",
    "\n",
    "- **Remove rows with missing values**:  \n",
    "  ```python\n",
    "  df.dropna(axis=0, inplace=True)\n",
    "  ```\n",
    "  This removes any row that contains at least one NaN.\n",
    "\n",
    "- **Remove columns with missing values**:\n",
    "  ```python\n",
    "  df.dropna(axis=1, inplace=True)\n",
    "  ```\n",
    "  This removes any column that contains at least one NaN.\n",
    "\n",
    "- **Conditional removal**: Remove rows if NaN appears in a specific column:\n",
    "  ```python\n",
    "  df.dropna(subset=['column_name'], inplace=True)\n",
    "  ```\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Imputation\n",
    "\n",
    "Imputation means filling in missing values with reasonable estimates. Common methods include:\n",
    "\n",
    "- **Fill with a constant value**:\n",
    "  ```python\n",
    "  df.fillna(0, inplace=True)  # or any constant\n",
    "  ```\n",
    "\n",
    "- **Fill with mean, median, or mode**:\n",
    "  ```python\n",
    "  df['column_name'].fillna(df['column_name'].mean(), inplace=True)\n",
    "  df['column_name'].fillna(df['column_name'].median(), inplace=True)\n",
    "  df['column_name'].fillna(df['column_name'].mode()[0], inplace=True)\n",
    "  ```\n",
    "\n",
    "- **Forward fill (propagate last valid value)**:\n",
    "  ```python\n",
    "  df.fillna(method='ffill', inplace=True)\n",
    "  ```\n",
    "\n",
    "- **Backward fill (propagate next valid value)**:\n",
    "  ```python\n",
    "  df.fillna(method='bfill', inplace=True)\n",
    "  ```\n",
    "\n",
    "- **Interpolation (linear, polynomial, etc.)**:\n",
    "  ```python\n",
    "  df['column_name'].interpolate(method='linear', inplace=True)\n",
    "  ```\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Checking for Missing Values\n",
    "\n",
    "Before handling NaNs, we can check for them using:\n",
    "\n",
    "- **Check if any NaNs exist**:\n",
    "  ```python\n",
    "  df.isna().any()\n",
    "  ```\n",
    "\n",
    "- **Count of NaNs per column**:\n",
    "  ```python\n",
    "  df.isna().sum()\n",
    "  ```\n",
    "\n",
    "- **Quick overview**:\n",
    "  ```python\n",
    "  df.info()\n",
    "  ```\n",
    "\n",
    "---\n",
    "\n",
    "By choosing the appropriate deletion or imputation method, we can prepare the dataset for further analysis or machine learning tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "844149ab",
   "metadata": {},
   "source": [
    "### Missing values dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e84da3d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Generate larger sample data with Address column full of NaNs\n",
    "data = {\n",
    "    'Name': ['Alice', 'Bob', 'Charlie', 'David', 'Eva', 'Frank', 'Grace', 'Hannah',\n",
    "             'Ian', 'Jack', 'Karen', 'Liam', 'Mona', 'Nina', 'Oscar', 'Paula'],\n",
    "    'Age': [25, np.nan, 30, 22, np.nan, 28, 35, 32, 29, np.nan, 40, 31, 27, np.nan, 33, 26],\n",
    "    'Salary': [50000, 54000, np.nan, 48000, 52000, np.nan, 60000, 58000,\n",
    "               51000, 53000, np.nan, 49000, 55000, 57000, np.nan, 50000],\n",
    "    'Department': ['HR', 'IT', 'Finance', np.nan, 'IT', 'HR', 'Finance', np.nan,\n",
    "                   'IT', 'Finance', 'HR', np.nan, 'Finance', 'HR', 'IT', np.nan],\n",
    "    'Address': [np.nan]*16  # Entire column is NaN\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8c65c91",
   "metadata": {},
   "source": [
    "##"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22343608",
   "metadata": {},
   "source": [
    "<h2>Take Home: Reading Data, Exploring Data Types, Properties and Basic Statistics in pandas</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b95fb0ae",
   "metadata": {},
   "source": [
    "**in the following instructions, you will learn techniques to work with Pandas dataframes, go through them and understand the value of their different functions in perparation for next lab session**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5764496",
   "metadata": {},
   "source": [
    "We import two of the most fundamental libraries for data mining: [pandas](https://pandas.pydata.org/docs/index.html) for data manipulation and [numpy](https://numpy.org/) for numerical computations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaccdf7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca4e83d4",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09917b8e",
   "metadata": {},
   "source": [
    "### The `read_csv()` Function in pandas\n",
    "We are using `pd.read_csv` to read two CSV files:\n",
    "- `country_wise_latest.csv`: This contains the latest country-wise statistics for COVID-19.\n",
    "<!-- - `day_wise.csv`: This file holds the day-wise statistics for global COVID-19 cases. -->\n",
    "\n",
    "Parameters used:\n",
    "- `pd.read_csv(file_path)`: Reads the CSV files from the specified path.\n",
    "- `sep=','`: Specifies the separator used to divide values in the file. The default value is a comma (`,`), which is used in standard CSV files like ours.\n",
    "\n",
    "Since commas are the default separator in `pd.read_csv()`, there is no need to specify this parameter unless a different separator is used (e.g., a tab `\\t` or semicolon `;`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d77e57d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading the CSV file\n",
    "df1 = pd.read_csv('country_wise_latest.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00c0914d",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6e5b9be",
   "metadata": {},
   "source": [
    "### The `head()` Function in pandas\n",
    "The `head()` function in pandas allows you to preview the first few rows of a DataFrame. By default, it shows the first 5 rows, but you can specify a different number of rows if needed.\n",
    "\n",
    "**Syntax**: `df.head(n)`  \n",
    "- `n` (optional): The number of rows to display. If not specified, the default is 5.\n",
    "\n",
    "This method is useful for quickly inspecting the top rows of a dataset to understand its structure and contents.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b23f6b42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the first few rows of both dataset\n",
    "df1.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "565004c7",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d7b05bb",
   "metadata": {},
   "source": [
    "### The `tail()` Function in pandas\n",
    "\n",
    "The `tail()` function in pandas allows you to view the last few rows of a DataFrame. By default, it returns the last 5 rows, but you can specify the number of rows to display.\n",
    "\n",
    "#### Syntax:\n",
    "`df.tail(n)`\n",
    "\n",
    "- **n** (optional): The number of rows to return. If not specified, the default is 5.\n",
    "\n",
    "This function is useful for quickly inspecting the final portion of a dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14bd1948",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21ec63cd",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c3436fe",
   "metadata": {},
   "source": [
    "### The `shape()` Function in pandas\n",
    "\n",
    "The `shape()` function in pandas returns a tuple representing the dimensions of a DataFrame. It provides two values:\n",
    "- The number of rows.\n",
    "- The number of columns.\n",
    "\n",
    "This is useful for quickly inspecting the size and structure of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0d407fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d799d2ba",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db36712e",
   "metadata": {},
   "source": [
    "### The `info()` Function in pandas\n",
    "The `info()` function in pandas provides a concise summary of a DataFrame, including:\n",
    "- The total number of entries (rows) and columns.\n",
    "- The names of all the columns and their data types.\n",
    "- The number of non-null values in each column.\n",
    "- Memory usage of the DataFrame.\n",
    "\n",
    "This method is useful for quickly inspecting the structure and contents of a DataFrame.\n",
    "\n",
    "Example:\n",
    "`df.info()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cddbabe",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d45b51b",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87e08f31",
   "metadata": {},
   "source": [
    "### The `type()` Function in pandas\n",
    "The `type()` method in Python returns the type of a given object. It can be used to check the type of a variable, a DataFrame, or any other object.\n",
    "\n",
    "For example:\n",
    "- `type(df)` will return `<class 'pandas.core.frame.DataFrame'>`, confirming that `df` is a DataFrame.\n",
    "- You can also use it on individual columns or any variable to determine their type.\n",
    "\n",
    "Example:\n",
    "`type(df['column_name'])`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f0575db",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(df1['WHO Region'][0]))\n",
    "type(df1['Country/Region'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4b2046b",
   "metadata": {},
   "source": [
    "### Why Does `type()` Return Different Outputs?\n",
    "\n",
    "In Jupyter notebooks, the `type()` function may return different outputs depending on how it's used:\n",
    "\n",
    "- **With `print()`**: The full class information is displayed, like `<class 'str'>`.\n",
    "  - Example: `print(type(df1['WHO Region'][0]))` will output `<class 'str'>`.\n",
    "  \n",
    "- **Without `print()`**: Jupyter automatically simplifies the output to just `str`.\n",
    "  - Example: `type(df1['Country/Region'][0])` will display `str`.\n",
    "\n",
    "To ensure consistent results, always use `print()` to display the output of `type()`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ac55a36",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2d5531d",
   "metadata": {},
   "source": [
    "### The `describe()` Function in pandas\n",
    "\n",
    "The `describe()` function in pandas generates descriptive statistics for numerical columns of a DataFrame, providing a summary of key statistical measures.\n",
    "\n",
    "#### Key Outputs:\n",
    "- **count**: The number of non-null (valid) entries in the column.\n",
    "- **mean**: The average (mean) value of the data in the column.\n",
    "- **std**: The standard deviation, a measure of how spread out the data is.\n",
    "- **min**: The minimum value in the column.\n",
    "- **25% (1st Quartile)**: The value below which 25% of the data falls.\n",
    "- **50% (Median)**: The middle value of the data.\n",
    "- **75% (3rd Quartile)**: The value below which 75% of the data falls.\n",
    "- **max**: The maximum value in the column.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac69bb4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbfd5c3b",
   "metadata": {},
   "source": [
    "### Explanation of `e+02` in `describe()`\n",
    "\n",
    "The `e+02` notation seen in the `describe()` output refers to scientific notation, which is used to express very large or very small numbers in a compact form. \n",
    "\n",
    "In scientific notation, `e+02` means \"multiply by \\(10^2\\)\" (or 100). For example:\n",
    "- `1.87e+02` is equivalent to `1.87 × 10^2 = 187`.\n",
    "- `8.81e+04` is equivalent to `8.81 × 10^4 = 88,130`.\n",
    "\n",
    "This notation is often used in pandas to make it easier to display large datasets in a more compact form, especially when numbers vary widely in magnitude.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65136c91",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52b598cf",
   "metadata": {},
   "source": [
    "### How to Bypass Scientific Notation\n",
    "\n",
    "If you want to disable scientific notation and display the full numbers in pandas, you can adjust the pandas display settings using `pd.set_option()`. This allows you to format the output with a specific number of decimal places. \n",
    "\n",
    "For example:\n",
    "```python\n",
    "pd.set_option('display.float_format', '{:.2f}'.format)\n",
    "\n",
    "The command `pd.set_option('display.float_format', '{:.2f}'.format)` modifies how floating-point numbers are displayed in pandas. It formats all floats to show 2 decimal places (`.2f`), ensuring consistent number presentation throughout the DataFrame without scientific notation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d3926bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.float_format', '{:.2f}'.format)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f20c144a",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04b92963",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4fcb3d1",
   "metadata": {},
   "source": [
    "### Explanation of `std (383318.66)` in the Confirmed Column\n",
    "\n",
    "The **std (383318.66)** represents the **standard deviation** of the confirmed cases. It indicates the spread of the data around the mean. A high standard deviation means there is a wide range in the number of confirmed cases across locations/countries, with significant variation from the average.\n",
    "\n",
    "\n",
    "### Explanation of Percentiles in the Confirmed Column\n",
    "\n",
    "- **25% (1114.00)**:  \n",
    "  This represents the **first quartile (Q1)**. It means that 25% of the locations/countries have confirmed cases **less than or equal to 1,114**, while 75% have more.\n",
    "\n",
    "- **50% (5059.00)**:  \n",
    "  This represents the **median (Q2)**, or the middle value of the dataset. Half of the locations/countries have confirmed cases **less than or equal to 5,059**, and the other half have more.\n",
    "\n",
    "- **75% (40460.50)**:  \n",
    "  This represents the **third quartile (Q3)**. It means that 75% of the locations/countries have confirmed cases **less than or equal to 40,460.50**, while 25% have more.\n",
    "\n",
    "These percentiles help to understand how the confirmed cases are distributed across different locations/countries in the dataset.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8c0ae8c",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6d3744b",
   "metadata": {},
   "source": [
    "### Mapping Statistics to Functions in pandas\n",
    "\n",
    "When calculating basic statistics for a specific column in pandas, you can use the following functions:\n",
    "\n",
    "- **Mean**: `<target_column>.mean()` — This calculates the average of the column.\n",
    "- **Standard Deviation**: `<target_column>.std()` — This measures how much the values deviate from the mean.\n",
    "- **Min**: `<target_column>.min()` — Returns the smallest value in the column.\n",
    "- **Max**: `<target_column>.max()` — Returns the largest value in the column.\n",
    "- **Quartiles**: \n",
    "  - **25th percentile (Q1)**: `<target_column>.quantile(0.25)` — The value below which 25% of the data falls.\n",
    "  - **50th percentile (Median)**: `<target_column>.median()` or `<target_column>.quantile(0.50)` — The middle value of the data.\n",
    "  - **75th percentile (Q3)**: `<target_column>.quantile(0.75)` — The value below which 75% of the data falls.\n",
    "\n",
    "These functions allow you to compute individual statistics for any column in your dataset. For example, you can apply these to the **New recovered** column to explore its central tendencies and spread."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a24aecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mean of the New recovered column\n",
    "mean_new_recovered = df1['New recovered'].mean()\n",
    "\n",
    "# Standard deviation of the New recovered column\n",
    "std_new_recovered = df1['New recovered'].std()\n",
    "\n",
    "# Minimum value of the New recovered column\n",
    "min_new_recovered = df1['New recovered'].min()\n",
    "\n",
    "# Maximum value of the New recovered column\n",
    "max_new_recovered = df1['New recovered'].max()\n",
    "\n",
    "# Quartiles\n",
    "q1_new_recovered = df1['New recovered'].quantile(0.25)\n",
    "q2_new_recovered = df1['New recovered'].median()\n",
    "q3_new_recovered = df1['New recovered'].quantile(0.75)\n",
    "\n",
    "# Display the results\n",
    "print(f\"Mean: {mean_new_recovered}\")\n",
    "print(f\"Standard Deviation: {std_new_recovered}\")\n",
    "print(f\"Minimum: {min_new_recovered}\")\n",
    "print(f\"Maximum: {max_new_recovered}\")\n",
    "print(f\"25% (Q1): {q1_new_recovered}\")\n",
    "print(f\"50% (Median): {q2_new_recovered}\")\n",
    "print(\"75% (Q3): \" + str(q3_new_recovered))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bc455e7",
   "metadata": {},
   "source": [
    "### Difference Between `print(f\"...\")` and `print(\"...\" + str(...))`\n",
    "\n",
    "1. **`print(f\"50% (Median): {q2_new_recovered}\")`**:\n",
    "   - This is **f-string** formatting, introduced in Python 3.6+. It allows you to directly embed variables within curly braces `{}` inside a string, making the code more concise and readable.\n",
    "\n",
    "2. **`print(\"75% (Q3): \" + str(q3_new_recovered))`**:\n",
    "   - This uses string concatenation. The `str()` function converts the variable to a string before combining it with another string using the `+` operator."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ef7a70e",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "578e546e",
   "metadata": {},
   "source": [
    "### The `isnull()` Function in pandas\n",
    "\n",
    "The `isnull()` function in pandas identifies missing (null or NaN) values in a DataFrame. It returns a DataFrame of the same shape, with `True` indicating missing values and `False` indicating non-missing values. This function is useful for detecting incomplete data, which is crucial for data cleaning and preprocessing. By understanding where data is missing, we can take appropriate steps, such as filling or removing those values, to ensure accurate analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f2509e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using isnull() to detect missing values\n",
    "missing_values = df1.isnull()\n",
    "\n",
    "# Display the DataFrame with True for missing values and False for non-missing values\n",
    "print(missing_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69e5f61a",
   "metadata": {},
   "source": [
    "### Output of `isnull()`\n",
    "\n",
    "The `isnull()` function returns a DataFrame of the same shape, where each cell contains either `True` or `False`. \n",
    "- **`True`** indicates that the value in the corresponding cell is missing (null or NaN).\n",
    "- **`False`** indicates that the value in the corresponding cell is not missing.\n",
    "\n",
    "This allows us to identify which parts of the DataFrame have missing data, helping in the data cleaning process.\n",
    "We can use it directly on a column too. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cd9b8c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1['Country/Region'].isnull()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2b4b63d",
   "metadata": {},
   "source": [
    "### Output Explanation of `isnull()`\n",
    "\n",
    "In this output, `isnull()` is applied to the **Country/Region** column, returning `True` for missing values and `False` for non-missing values. However, only the first few and last few rows are displayed, with the middle rows omitted (indicated by `...`). As a result, while we can see that the first and last rows contain no missing values, the state of the rows in between is not immediately visible from this output.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f89a883",
   "metadata": {},
   "source": [
    "### How to Get the Count of Missing Values per Column\n",
    "\n",
    "When using `isnull()`, it returns `True` or `False` for each cell, but it doesn't summarize how many missing values exist per column. To address this, we use `isnull().sum()` to get the total number of missing values for each column. This gives a clearer breakdown of missing data, helping us assess data quality and decide how to handle missing values for each column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0211042",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77b1067b",
   "metadata": {},
   "source": [
    "### Explanation of `df_world.isnull().sum()`\n",
    "\n",
    "The command `df_world.isnull().sum()` checks for missing (null) values in the DataFrame `df_world`. \n",
    "\n",
    "- **`isnull()`**: Identifies whether each element is missing (`True` for missing, `False` for non-missing).\n",
    "- **`sum()`**: Adds up the number of `True` values (which represent null values) for each column.\n",
    "\n",
    "The result is the total count of missing values for each column in the DataFrame.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "279b240c",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c08f6cba",
   "metadata": {},
   "source": [
    "### The `duplicated()` Function in pandas\n",
    "\n",
    "The `duplicated()` function in pandas identifies duplicate rows in a DataFrame or Series. It returns a boolean Series where:\n",
    "- **`True`** indicates that the row is a duplicate (i.e., it has appeared before).\n",
    "- **`False`** indicates that the row is unique.\n",
    "\n",
    "By default, it checks for duplicates across all columns, but you can specify columns or control whether the first or last occurrence is marked as duplicate.\n",
    "\n",
    "This function is useful for identifying and handling repeated data entries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d402ee09",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.duplicated()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b353bfe7",
   "metadata": {},
   "source": [
    "### Common Issue with `duplicated()` Output\n",
    "\n",
    "Similar to `isnull()`, the `duplicated()` function returns a Series of `True` or `False` values for each row, indicating whether it's a duplicate. However, this output alone doesn't tell us how many duplicates are present in each column. To solve this, we can use `duplicated().sum()` to get the total count of duplicate rows in the dataset. This gives a clear summary of how many rows are duplicated.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc7cd10f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.duplicated().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4999bd7",
   "metadata": {},
   "source": [
    "### Explanation of `df1.duplicated().sum()`\n",
    "\n",
    "Running `df1.duplicated().sum()` provides the total count of duplicated rows in the DataFrame. It identifies duplicate rows and then sums them to give the number of repeated entries, offering a quick overview of data redundancy in the dataset.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4d754aa",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afd88506",
   "metadata": {},
   "source": [
    "<h2>Let's load another csv file</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9980e5c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.read_csv('covid_19_clean_complete.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5f7f43d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the first few rows of the second dataset\n",
    "df2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ea881a0",
   "metadata": {},
   "source": [
    "### Purpose of `fillna()` in pandas\n",
    "\n",
    "The `fillna()` function in pandas is used to replace missing values (NaN) in a DataFrame or Series with a specified value. This is essential for handling incomplete data, as it allows you to fill gaps in the dataset with meaningful or placeholder values (e.g., a space `' '`, zero, or a specific string). Filling NaN values helps avoid errors in further analysis or calculations by ensuring that no missing data is left untreated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8000b37d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3088a7a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2['Province/State'] = df2['Province/State'].fillna(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dcd7a24",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f297edb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2['Province/State'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4226e091",
   "metadata": {},
   "source": [
    "### Explanation of `fillna()` command\n",
    "\n",
    "In this command, `df['Province/State'].fillna(' ', inplace=True)`, we are replacing all missing values (NaN) in the **Province/State** column with a single space `' '`.\n",
    "\n",
    "- **`' '`**: This fills any missing values with a space, ensuring that there are no NaN values left in the column.\n",
    "- **`inplace=True`**: This ensures the changes are made directly in the DataFrame without creating a copy, meaning the original DataFrame is updated.\n",
    "\n",
    "This helps to clean missing values in the **Province/State** column.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd42b8db",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2fdea92",
   "metadata": {},
   "source": [
    "<h4>Let's load csv file again</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c210a442",
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = pd.read_csv('country_wise_latest.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16ba94c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df3.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f707be3b",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50977886",
   "metadata": {},
   "source": [
    "### The `columns` function in pandas\n",
    "\n",
    "The `columns` function in pandas is used to return the column labels of a DataFrame. It provides an Index object containing the column names in the order they appear in the DataFrame. This is useful for quickly inspecting the structure of the DataFrame, accessing specific columns, or renaming them if needed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7039dc50",
   "metadata": {},
   "outputs": [],
   "source": [
    "df3.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a66f08ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86cfdcc8",
   "metadata": {},
   "source": [
    "### Visual Comparison of Common Columns\n",
    "\n",
    "As we can see, there are some common columns between the two datasets. Identifying these common columns will allow us to merge or compare the datasets more effectively. Let's proceed by finding out exactly which columns are shared between both datasets.\n",
    "<br />Let's find the common columns between the two datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e378fa8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#finding the common columns in between the two datasets that we'd be analysing\n",
    "df2_columns = set(df2.columns)\n",
    "df3_columns = set(df3.columns)\n",
    "common_columns = df2_columns & df3_columns\n",
    "print(common_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2c86af2",
   "metadata": {},
   "source": [
    "### Explanation of the Code\n",
    "\n",
    "In this code, we are comparing the columns of two datasets (`df2` and `df3`) to find the common columns:\n",
    "\n",
    "1. **`set(df2.columns)` and `set(df3.columns)`**: Convert the columns of each DataFrame into a set. A set is an unordered collection of unique elements, allowing us to easily perform set operations like finding common elements.\n",
    "   \n",
    "2. **`&` (AND operator)**: This operator is used to find the intersection of two sets, meaning it returns the columns that are common between both DataFrames.\n",
    "\n",
    "The result, `common_columns`, will display the shared columns between `df2` and `df3`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6666e6cd",
   "metadata": {},
   "source": [
    "### Merging the Two DataFrames\n",
    "\n",
    "Now that we've identified the common columns—'Confirmed', 'WHO Region', 'Recovered', 'Deaths', 'Active', and 'Country/Region'—between the two datasets, we can proceed to merge them. By using these shared columns, particularly `Country/Region`, we can combine the data into a single DataFrame for a more comprehensive analysis. Merging will allow us to consolidate the information from both datasets while maintaining the structure of the common columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "128fa2ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "common = list(common_columns)\n",
    "merged = pd.merge(df2, df3, on=common, how='inner')\n",
    "merged.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "365d19ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59f63059",
   "metadata": {},
   "source": [
    "### Explanation of Converting a Set to a List\n",
    "\n",
    "In the code:\n",
    "common = list(common_columns)\n",
    "\n",
    "We convert `common_columns`, which is a set of column names shared between two DataFrames, into a list using `list(common_columns)`. This is necessary because the `pd.merge()` function requires a list of column names to specify which columns to merge on. While sets store unique elements, lists preserve the order of the columns and are compatible with pandas functions like `merge()`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "072bc15a",
   "metadata": {},
   "source": [
    "### Explanation of the `merge()` Function\n",
    "\n",
    "In the code:\n",
    "merged = pd.merge(df2, df3, on=common, how='inner')\n",
    "\n",
    "The `pd.merge()` function merges two DataFrames (`df2` and `df3`) based on the common columns:\n",
    "\n",
    "- **`on=common`**: Specifies the columns used for the merge.\n",
    "- **`how='inner'`**: An inner join includes only rows where the values in the common columns match in both DataFrames. Rows without matches in either DataFrame are excluded.\n",
    "\n",
    "For example, if a <u><i>country value</i></u> (e.g. Germany) in the `Country/Region` column is present in `df2` but not in `df3`, <u><i>that row</i></u> will be excluded from the final merged DataFrame."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9686f141",
   "metadata": {},
   "source": [
    "### Explanation of Other `how` Options in `merge()`\n",
    "\n",
    "In addition to `how='inner'`, there are other join options for merging DataFrames:\n",
    "\n",
    "- **`how='outer'`**: Performs an outer join, keeping all rows from both DataFrames. Missing values are filled with NaN where there is no match.\n",
    "- **`how='left'`**: Performs a left join, keeping all rows from the left DataFrame and adding matching rows from the right. The left DataFrame is the first DataFrame passed to `pd.merge()`, while the right DataFrame is the second one.\n",
    "- **`how='right'`**: Performs a right join, keeping all rows from the right DataFrame and adding matching rows from the left. The right DataFrame is the second one passed to `pd.merge()`.\n",
    "\n",
    "For more information, refer to the [pandas documentation](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.merge.html).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e045c02f",
   "metadata": {},
   "source": [
    "# The End"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
